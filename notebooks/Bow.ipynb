{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import pandas\n",
    "import tqdm\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "import unidecode\n",
    "import contractions\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import Counter\n",
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "from read_utils import read_file, create_folder, temp_record_query, temp_record_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 20:21:45 WARN Utils: Your hostname, Runyus-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.3.12 instead (on interface en0)\n",
      "22/10/07 20:21:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/07 20:21:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/07 20:21:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/07 20:21:46 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/10/07 20:21:46 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    # Create a spark session (which will run spark jobs)\n",
    "    SparkSession.builder.appName(\"Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config('spark.executor.memory','10g')\n",
    "    .config('spark.driver.memory','12g')\n",
    "    .config('spark.driver.maxResultsSize', '10GiB')\n",
    "    # .config(\"spark.network.timeout\", \"3600s\")\n",
    "    # .master(\"local[6]\")\n",
    "    .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Loading File...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Loading Finished!\n",
      "-RECORD 0-----------------------------------------------------------------------------------------\n",
      " merchant_name | Felis Limited                                                                    \n",
      " tags          | furniture, home furnishings and equipment shops, and manufacturers, except ap... \n",
      " merchant_abn  | 10023283211                                                                      \n",
      " take_rate     | 0.18                                                                             \n",
      " type          | e                                                                                \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = read_file(spark, 'merchants_data.parquet', '../data/curated/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean string data\n",
    "\n",
    "# Convert accented characters  return:string\n",
    "def accented_char(text):\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "# expand_contractions return:string\n",
    "def expand_contrac(text):\n",
    "    text = contractions.fix(text)\n",
    "    return text\n",
    "\n",
    "# remove num and symbol return: string\n",
    "def replace_num_and_symbol(text):\n",
    "    text = text.lower()\n",
    "    result = re.sub('[\\W_]+', ' ', text)\n",
    "    new_string = re.sub(r'([\\d]+)([a-z]+)', '', result)\n",
    "    new_string = re.sub(r'([a-z]+)([\\d]+)', '', new_string)\n",
    "    new_string = re.sub(r'[\\d]+', '', new_string)\n",
    "    return new_string\n",
    "\n",
    "# remove stopwords \n",
    "def remove_stop_words(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    token_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "    # filter_sentence = ''.join(token_without_sw)\n",
    "    return token_without_sw\n",
    "\n",
    "# Lemmatisation\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def word_lemmatizer(data_list):\n",
    "    # tokens = word_tokenize(text)\n",
    "    tagged_sent = pos_tag(data_list)\n",
    "    lem_list = WordNetLemmatizer()\n",
    "    word_list = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        word_list.append(lem_list.lemmatize(tag[0], pos=wordnet_pos))\n",
    "    return word_list\n",
    "\n",
    "def clean_string(text):\n",
    "    # using all functions above to clean string\n",
    "    remove_accent_char = accented_char(text)\n",
    "    remove_extraction = expand_contrac(remove_accent_char)\n",
    "    remove_num_and_symbol_string = replace_num_and_symbol(remove_extraction)\n",
    "    filter_sentence = remove_stop_words(remove_num_and_symbol_string)\n",
    "    word_list = word_lemmatizer(filter_sentence)\n",
    "    clean_string = ' '.join(word_list)\n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select('merchant_name').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Felis Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arcu Ac Orci Corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nunc Sed Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ultricies Dignissim Lacus Foundation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Enim Condimentum PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>Elit Dictum Eu Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>Mollis LLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023</th>\n",
       "      <td>Sociosqu Corp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024</th>\n",
       "      <td>Commodo Hendrerit LLC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>Massa PC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4026 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             merchant_name\n",
       "0                            Felis Limited\n",
       "1                 Arcu Ac Orci Corporation\n",
       "2                         Nunc Sed Company\n",
       "3     Ultricies Dignissim Lacus Foundation\n",
       "4                      Enim Condimentum PC\n",
       "...                                    ...\n",
       "4021                    Elit Dictum Eu Ltd\n",
       "4022                            Mollis LLP\n",
       "4023                        Sociosqu Corp.\n",
       "4024                 Commodo Hendrerit LLC\n",
       "4025                              Massa PC\n",
       "\n",
       "[4026 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_info = [x[0] for x in sdf[['merchant_name']].values]\n",
    "word_list = []\n",
    "for item in text_info:\n",
    "    text_tokens = word_tokenize(clean_string(item))\n",
    "    for word in text_tokens:\n",
    "        word_list.append(word)\n",
    "word_counter = Counter(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_counter = sorted(word_counter.items(), key=lambda d: d[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_PATH = '../plots/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/runyuyang/Downloads/2022S2/generic-buy-now-pay-later-project-group-24/plots/basic_wordcloud.html'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyecharts.options as opts\n",
    "from pyecharts.charts import WordCloud\n",
    "\"\"\"\n",
    "Gallery 使用 pyecharts 1.1.0\n",
    "参考地址: https://gallery.echartsjs.com/editor.html?c=xS1jMxuOVm\n",
    "\n",
    "目前无法实现的功能:\n",
    "\n",
    "1、暂无\n",
    "\"\"\"\n",
    "(\n",
    "    WordCloud()\n",
    "    .add(series_name=\"merchants_name_word\", data_pair=new_counter, word_size_range=[6, 66])\n",
    "    .set_global_opts(\n",
    "        title_opts=opts.TitleOpts(\n",
    "            title=\"Analysis_of_merchants_name_words\", title_textstyle_opts=opts.TextStyleOpts(font_size=23)\n",
    "        ),\n",
    "        tooltip_opts=opts.TooltipOpts(is_show=True),\n",
    "    )\n",
    "    .render(f\"{PLOT_PATH}basic_wordcloud.html\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
