{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(spark_session: SparkSession, file_name, ptw='../data/tables/', type='parquet', truncate=80, sep=','):\n",
    "    '''\n",
    "To read different type of file use spark. And show the first metadata after read. \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "spark_session : DataFrame\n",
    "file_name: str\n",
    "    The full name of the file to read. \n",
    "ptw : \n",
    "    The relative path of the file to read, default '../data/tables/'\n",
    "type : {'parquet', 'csv'}, default 'parquet'\n",
    "truncate : int, default 80\n",
    "    Parameter of `show` function spark dataframe, which controll the maximum \n",
    "    number of characters per row.\n",
    "sep : str, default ','\n",
    "    For csv reading, control the seperate character.\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "Spark DataFrame\n",
    "    A DataFrame of the read file.\n",
    "\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> sdf = read_file(spark, 'tbl_merchants.parquet')\n",
    "|> Loading File...\n",
    "|> Loading Finished!\n",
    "-RECORD 0----------------------------------------------------------------------------------------\n",
    " name         | Felis Limited                                                                    \n",
    " tags         | ((furniture, home furnishings and equipment shops, and manufacturers, except ... \n",
    " merchant_abn | 10023283211                                                                      \n",
    "only showing top 1 row\n",
    "\n",
    ">>> sdf = read_file(spark, 'tbl_merchants.parquet', truncate=20)\n",
    "|> Loading File...\n",
    "|> Loading Finished!\n",
    "-RECORD 0----------------------------\n",
    " name         | Felis Limited        \n",
    " tags         | ((furniture, home... \n",
    " merchant_abn | 10023283211          \n",
    "only showing top 1 row\n",
    "\n",
    ">>> sdf = read_file(spark, 'tbl_consumer.csv', type='csv', sep='|')\n",
    "|> Loading File...\n",
    "|> Loading Finished!\n",
    "-RECORD 0---------------------------------\n",
    " name        | Yolanda Williams           \n",
    " address     | 413 Haney Gardens Apt. 742 \n",
    " state       | WA                         \n",
    " postcode    | 6935                       \n",
    " gender      | Female                     \n",
    " consumer_id | 1195503                    \n",
    "only showing top 1 row\n",
    "    '''\n",
    "\n",
    "    # read file\n",
    "    print('|> Loading File...')\n",
    "    if type == 'csv':\n",
    "        sdf = spark_session.read.csv(f'{ptw}{file_name}', sep=sep, header=True)\n",
    "\n",
    "    elif type == 'parquet':\n",
    "        sdf = spark_session.read.parquet(f'{ptw}{file_name}')\n",
    "    print('|> Loading Finished!')\n",
    "\n",
    "    # print the first row of data \n",
    "    sdf.show(1, vertical=True, truncate=truncate)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/31 10:56:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "|> Loading File...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Loading Finished!\n",
      "-RECORD 0----------------------------------------------------------------------------------------\n",
      " name         | Felis Limited                                                                    \n",
      " tags         | ((furniture, home furnishings and equipment shops, and manufacturers, except ... \n",
      " merchant_abn | 10023283211                                                                      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    # Create a spark session (which will run spark jobs)\n",
    "    SparkSession.builder.appName(\"Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config('spark.executor.memory','10g')\n",
    "    .config('spark.driver.memory','12g')\n",
    "    .config('spark.driver.maxResultsSize', '10 GiB')\n",
    "    .config('spark.shuffle.file.buffer', '64k')\n",
    "    # .config(\"spark.network.timeout\", \"3600s\")\n",
    "    # .master(\"local[6]\")\n",
    "    .getOrCreate()\n",
    "    )\n",
    "sdf = read_file(spark, 'tbl_merchants.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Loading File...\n",
      "|> Loading Finished!\n",
      "-RECORD 0--------------\n",
      " user_id     | 1       \n",
      " consumer_id | 1195503 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = read_file(spark, 'consumer_user_details.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Loading File...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Loading Finished!\n",
      "-RECORD 0----------------------------------------------\n",
      " user_id        | 18478                                \n",
      " merchant_abn   | 62191208634                          \n",
      " dollar_value   | 63.255848959735246                   \n",
      " order_id       | 949a63c8-29f7-4ab0-ada4-99ac50a88952 \n",
      " order_datetime | 2021-08-20                           \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = read_file(spark, 'transactions_20210228_20210827_snapshot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(path):\n",
    "    '''\n",
    "Create folder.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "path : str\n",
    "    The relative path of the new folder. \n",
    "\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> create_folder('../data/temp')\n",
    "|> Create Successfully!\n",
    "\n",
    ">>> create_folder('../data/tables/consumer_user_details.parquet')\n",
    "|> The folder name duplicated with a file!\n",
    "|> Files already exist under the upper folder:\n",
    "   ['transactions_20210228_20210827_snapshot', '.DS_Store', '.gitkeep', 'consumer_user_details.parquet', 'tbl_consumer.csv', 'tbl_merchants.parquet']\n",
    "\n",
    ">>> create_folder('../data/tables')\n",
    "|> The folder already exist!\n",
    "|> Files already exist under this folder:\n",
    "   ['transactions_20210228_20210827_snapshot', '.DS_Store', '.gitkeep', 'consumer_user_details.parquet', 'tbl_consumer.csv', 'tbl_merchants.parquet']\n",
    "    '''\n",
    "\n",
    "    # folder should not already exist\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print('|> Create Successfully!')\n",
    "    \n",
    "    # if the folder aleady created, the print out the files under this folder\n",
    "    elif os.path.isdir(path):\n",
    "        print(f'|> The folder already exist!\\n|> Files already exist under this folder:\\n   {os.listdir(path)}')\n",
    "    \n",
    "    # the name of the new folder is the same as a file already exist under the upper folder\n",
    "    elif os.path.isfile(path):\n",
    "        upper_path = '/'.join(path.split('/')[:-1])\n",
    "        print(f'|> The folder name duplicated with a file!\\n|> Files already exist under the upper folder:\\n   {os.listdir( upper_path )}')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Create Successfully!\n"
     ]
    }
   ],
   "source": [
    "create_folder('../data/temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temp_record_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_record_sdf(sdf:SparkSession, path = '../data/temp', overwrite = False):\n",
    "    '''\n",
    "Save current progress for future steps\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "sdf : spark dataframe\n",
    "path : str\n",
    "    Path to save data, defualt as `../data/temp`\n",
    "overwrite : bool\n",
    "    Set if cover the origin data, defualt False\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> temp_record_sdf(sdf, path='../data/temp')\n",
    ">>> temp_record_sdf(sdf, path='../data/temp')\n",
    ">>> temp_record_sdf(sdf, path='../data/temp', overwrite=True)\n",
    "|> Waitting for saving...\n",
    "|> Save Successfully!\n",
    "--\n",
    "|> Waitting for saving...\n",
    "|> The folder already exist! Change the attr `overwrite` to cover the origin data.\n",
    "-- \n",
    "|> Waitting for saving...\n",
    "|> Save Successfully!\n",
    "\n",
    ">>> print(os.listdir( '../data' ))\n",
    ">>> print(os.path.isfile( '../data/temp.parquet' ))\n",
    ">>> temp_record_sdf(sdf, path='../data/temp.parquet')\n",
    ">>> temp_record_sdf(sdf, path='../data/temp.parquet', overwrite=True)\n",
    "['tables', '.gitkeep', 'README.md', 'temp.parquet', 'curated']\n",
    "--\n",
    "True\n",
    "--\n",
    "|> The name duplicated with a file!\n",
    "   Change the name or change the attr `overwrite` to cover the origin data.\n",
    "--\n",
    "|> Waitting for saving...\n",
    "|> Save Successfully!\n",
    "    '''\n",
    "\n",
    "\n",
    "    # folder should not already exist\n",
    "    if not os.path.exists(path):\n",
    "        print('|> Waitting for saving...')\n",
    "        sdf.write.parquet(path)\n",
    "        print('|> Save Successfully!')\n",
    "    \n",
    "    # if the folder aleady created, the print out the files under this folder\n",
    "    elif os.path.isdir(path):\n",
    "        try:\n",
    "            print('|> Waitting for saving...')\n",
    "            if (overwrite):\n",
    "                sdf.write.partitionBy('order_datetime').parquet(path, mode = 'overwrite')\n",
    "            else:\n",
    "                sdf.write.parquet(path)\n",
    "            print('|> Save Successfully!')\n",
    "        except Exception:\n",
    "            print('|> The folder already exist! Change the attr `overwrite` to cover the origin data.')\n",
    "    \n",
    "    # the name of the new folder is the same as a file already exist under the upper folder\n",
    "    elif os.path.isfile(path):\n",
    "        if (overwrite):\n",
    "            print('|> Waitting for saving...')\n",
    "            sdf.write.parquet(path, mode = 'overwrite')\n",
    "            print('|> Save Successfully!')\n",
    "        else:\n",
    "            print(f'|> The name duplicated with a file!\\n   Change the name or change the attr `overwrite` to cover the origin data.')\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- dollar_value: double (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# DataFrame转换成RDD\n",
    "result = sdf.rdd.map ( lambda p : \"user_id: \" + str(p.user_id)+ \" merchant_abn: \" + str(p.merchant_abn) + \\\n",
    "    \" dollar_value: \" + str(p.dollar_value) + \" order_id: \" + \\\n",
    "    p.order_id + \" order_datetime: \" + str(p.order_datetime) ).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Waitting for saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/31 10:18:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "|> Save Successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_record_sdf(sdf, path='../data/temp')\n",
    "# temp_record_sdf(sdf, path='../data/temp')\n",
    "# temp_record_sdf(sdf, path='../data/temp', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temp_record_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_record_query(sql_query:SparkSession.sql, *cols,\\\n",
    "    path='../data/temp', overwrite = False):\n",
    "    '''\n",
    "Save current progress for future steps\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "sql_query : spark sql query\n",
    "*cols : 'ColumnsOrName'\n",
    "    Name of columns.\n",
    "path : str\n",
    "    Path to save data, defualt as `../data/temp`\n",
    "overwrite : bool\n",
    "    Set if cover the origin data, defualt False\n",
    "\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> sql_query = sdf.orderBy('merchant_abn')\n",
    ">>> temp_record_query(sql_query, 'name', 'tags', 'merchant_abn')\n",
    "|> Waitting for saving...\n",
    "|> Save Successfully!\n",
    "    '''\n",
    "    # convert to spark dataframe and save\n",
    "    temp_record_sdf(sql_query.toDF(*cols), path=path, overwrite=overwrite)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Waitting for saving...\n",
      "|> Save Successfully!\n"
     ]
    }
   ],
   "source": [
    "sql_query = sdf.orderBy('merchant_abn')\n",
    "temp_record_query(sql_query, 'name', 'tags', 'merchant_abn', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transfor between DataFrame and RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# DataFrame to RDD\n",
    "srdd = sdf.rdd.map ( lambda p: Row(user_id=p.user_id, merchant_abn=p.merchant_abn, \\\n",
    "    dollar_value=p.dollar_value, order_id=p.order_id, order_datetime=p.order_datetime) )\n",
    "\n",
    "# RDD to DataFrame\n",
    "sdf = spark.createDataFrame( srdd )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
