{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(spark_session: SparkSession, file_name, ptw='../data/tables/', type='parquet', truncate=80, sep=','):\n",
    "    '''\n",
    "To read different type of file use spark. And show the first metadata after read. \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "spark_session : DataFrame\n",
    "file_name: str\n",
    "    The full name of the file to read. \n",
    "ptw : \n",
    "    The relative path of the file to read, default '../data/tables/'\n",
    "type : {'parquet', 'csv'}, default 'parquet'\n",
    "truncate : int, default 80\n",
    "    Parameter of `show` function spark dataframe, which control the maximum \n",
    "    number of characters per row.\n",
    "sep : str, default ','\n",
    "    For csv reading, control the seperate character.\n",
    "\n",
    "\n",
    "Returns\n",
    "-------\n",
    "Spark DataFrame\n",
    "    A DataFrame of the read file.\n",
    "\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> sdf = read_file(spark, 'tbl_merchants.parquet')\n",
    "|> Loading File...\n",
    "|> Loading Finished!\n",
    "-RECORD 0----------------------------------------------------------------------------------------\n",
    " name         | Felis Limited                                                                    \n",
    " tags         | ((furniture, home furnishings and equipment shops, and manufacturers, except ... \n",
    " merchant_abn | 10023283211                                                                      \n",
    "only showing top 1 row\n",
    "\n",
    ">>> sdf = read_file(spark, 'tbl_merchants.parquet', truncate=20)\n",
    "|> Loading File...\n",
    "|> Loading Finished!\n",
    "-RECORD 0----------------------------\n",
    " name         | Felis Limited        \n",
    " tags         | ((furniture, home... \n",
    " merchant_abn | 10023283211          \n",
    "only showing top 1 row\n",
    "\n",
    ">>> sdf = read_file(spark, 'tbl_consumer.csv', type='csv', sep='|')\n",
    "|> Loading File...\n",
    "|> Loading Finished!\n",
    "-RECORD 0---------------------------------\n",
    " name        | Yolanda Williams           \n",
    " address     | 413 Haney Gardens Apt. 742 \n",
    " state       | WA                         \n",
    " postcode    | 6935                       \n",
    " gender      | Female                     \n",
    " consumer_id | 1195503                    \n",
    "only showing top 1 row\n",
    "    '''\n",
    "\n",
    "    # read file\n",
    "    print('|> Loading File...')\n",
    "    if type == 'csv':\n",
    "        sdf = spark_session.read.csv(f'{ptw}{file_name}', sep=sep, header=True)\n",
    "\n",
    "    elif type == 'parquet':\n",
    "        sdf = spark_session.read.parquet(f'{ptw}{file_name}')\n",
    "    print('|> Loading Finished!')\n",
    "\n",
    "    # print the first row of data \n",
    "    sdf.show(1, vertical=True, truncate=truncate)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Loading File...\n",
      "|> Loading Finished!\n",
      "-RECORD 0----------------------------------------------------------------------------------------\n",
      " name         | Felis Limited                                                                    \n",
      " tags         | ((furniture, home furnishings and equipment shops, and manufacturers, except ... \n",
      " merchant_abn | 10023283211                                                                      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    # Create a spark session (which will run spark jobs)\n",
    "    SparkSession.builder.appName(\"Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config('spark.executor.memory','10g')\n",
    "    .config('spark.driver.memory','12g')\n",
    "    .config('spark.driver.maxResultsSize', '10 GiB')\n",
    "    .config('spark.shuffle.file.buffer', '64k')\n",
    "    # .config(\"spark.network.timeout\", \"3600s\")\n",
    "    # .master(\"local[6]\")\n",
    "    .getOrCreate()\n",
    "    )\n",
    "sdf = read_file(spark, 'tbl_merchants.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+\n",
      "|                name|                tags|merchant_abn|\n",
      "+--------------------+--------------------+------------+\n",
      "|       Felis Limited|((furniture, home...| 10023283211|\n",
      "|Arcu Ac Orci Corp...|([cable, satellit...| 10142254217|\n",
      "|    Nunc Sed Company|([jewelry, watch,...| 10165489824|\n",
      "|Ultricies Digniss...|([wAtch, clock, a...| 10187291046|\n",
      "| Enim Condimentum PC|([music shops - m...| 10192359162|\n",
      "|       Fusce Company|[(gift, card, nov...| 10206519221|\n",
      "|Aliquam Enim Inco...|[(computers, comP...| 10255988167|\n",
      "|    Ipsum Primis Ltd|[[watch, clock, a...| 10264435225|\n",
      "|Pede Ultrices Ind...|([computer progra...| 10279061213|\n",
      "|           Nunc Inc.|[(furniture, home...| 10323485998|\n",
      "+--------------------+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Loading File...\n",
      "|> Loading Finished!\n",
      "-RECORD 0--------------\n",
      " user_id     | 1       \n",
      " consumer_id | 1195503 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = read_file(spark, 'consumer_user_details.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|user_id|consumer_id|\n",
      "+-------+-----------+\n",
      "|      1|    1195503|\n",
      "|      2|     179208|\n",
      "|      3|    1194530|\n",
      "|      4|     154128|\n",
      "|      5|     712975|\n",
      "|      6|     407340|\n",
      "|      7|     511685|\n",
      "|      8|     448088|\n",
      "|      9|     650435|\n",
      "|     10|    1058499|\n",
      "+-------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Loading File...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Loading Finished!\n",
      "-RECORD 0----------------------------------------------\n",
      " user_id        | 18478                                \n",
      " merchant_abn   | 62191208634                          \n",
      " dollar_value   | 63.255848959735246                   \n",
      " order_id       | 949a63c8-29f7-4ab0-ada4-99ac50a88952 \n",
      " order_datetime | 2021-08-20                           \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = read_file(spark, 'transactions_20210228_20210827_snapshot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------------+--------------------+--------------+\n",
      "|user_id|merchant_abn|      dollar_value|            order_id|order_datetime|\n",
      "+-------+------------+------------------+--------------------+--------------+\n",
      "|  18478| 62191208634|63.255848959735246|949a63c8-29f7-4ab...|    2021-08-20|\n",
      "|      2| 15549624934| 130.3505283105634|6a84c3cf-612a-457...|    2021-08-20|\n",
      "|  18479| 64403598239|120.15860593212783|b10dcc33-e53f-425...|    2021-08-20|\n",
      "|      3| 60956456424| 136.6785200286976|0f09c5a5-784e-447...|    2021-08-20|\n",
      "|  18479| 94493496784| 72.96316578355305|f6c78c1a-4600-4c5...|    2021-08-20|\n",
      "|      3| 76819856970|  448.529684285612|5ace6a24-cdf0-4aa...|    2021-08-20|\n",
      "|  18479| 67609108741|  86.4040605836911|d0e180f0-cb06-42a...|    2021-08-20|\n",
      "|      3| 34096466752| 301.5793450525113|6fb1ff48-24bb-4f9...|    2021-08-20|\n",
      "|  18482| 70501974849| 68.75486276223054|8505fb33-b69a-412...|    2021-08-20|\n",
      "|      4| 49891706470| 48.89796461900801|ed11e477-b09f-4ae...|    2021-08-20|\n",
      "+-------+------------+------------------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Loading File...\n",
      "|> Loading Finished!\n",
      "-RECORD 0---------------------------------\n",
      " name        | Yolanda Williams           \n",
      " address     | 413 Haney Gardens Apt. 742 \n",
      " state       | WA                         \n",
      " postcode    | 6935                       \n",
      " gender      | Female                     \n",
      " consumer_id | 1195503                    \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file = read_file(spark, 'tbl_consumer.csv', type='csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-----+--------+------+-----------+\n",
      "|             name|             address|state|postcode|gender|consumer_id|\n",
      "+-----------------+--------------------+-----+--------+------+-----------+\n",
      "| Yolanda Williams|413 Haney Gardens...|   WA|    6935|Female|    1195503|\n",
      "|       Mary Smith|     3764 Amber Oval|  NSW|    2782|Female|     179208|\n",
      "|    Jill Jones MD|  40693 Henry Greens|   NT|     862|Female|    1194530|\n",
      "|  Lindsay Jimenez|00653 Davenport C...|  NSW|    2780|Female|     154128|\n",
      "|Rebecca Blanchard|9271 Michael Mano...|   WA|    6355|Female|     712975|\n",
      "|    Karen Chapman|2706 Stewart Oval...|  NSW|    2033|Female|     407340|\n",
      "|     Andrea Jones|   122 Brandon Cliff|  QLD|    4606|Female|     511685|\n",
      "| Stephen Williams|6804 Wright Crest...|   WA|    6056|  Male|     448088|\n",
      "|  Stephanie Reyes|5813 Denise Land ...|  NSW|    2482|Female|     650435|\n",
      "| Jillian Gonzales|461 Ryan Common S...|  VIC|    3220|Female|    1058499|\n",
      "+-----------------+--------------------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(path):\n",
    "    '''\n",
    "Create folder.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "path : str\n",
    "    The relative path of the new folder. \n",
    "\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> create_folder('../data/temp')\n",
    "|> Create Successfully!\n",
    "\n",
    ">>> create_folder('../data/tables/consumer_user_details.parquet')\n",
    "|> The folder name duplicated with a file!\n",
    "|> Files already exist under the upper folder:\n",
    "   ['transactions_20210228_20210827_snapshot', '.DS_Store', '.gitkeep', 'consumer_user_details.parquet', 'tbl_consumer.csv', 'tbl_merchants.parquet']\n",
    "\n",
    ">>> create_folder('../data/tables')\n",
    "|> The folder already exist!\n",
    "|> Files already exist under this folder:\n",
    "   ['transactions_20210228_20210827_snapshot', '.DS_Store', '.gitkeep', 'consumer_user_details.parquet', 'tbl_consumer.csv', 'tbl_merchants.parquet']\n",
    "    '''\n",
    "\n",
    "    # folder should not already exist\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print('|> Create Successfully!')\n",
    "    \n",
    "    # if the folder aleady created, the print out the files under this folder\n",
    "    elif os.path.isdir(path):\n",
    "        print(f'|> The folder already exist!\\n|> Files already exist under this folder:\\n   {os.listdir(path)}')\n",
    "    \n",
    "    # the name of the new folder is the same as a file already exist under the upper folder\n",
    "    elif os.path.isfile(path):\n",
    "        upper_path = '/'.join(path.split('/')[:-1])\n",
    "        print(f'|> The folder name duplicated with a file!\\n|> Files already exist under the upper folder:\\n   {os.listdir( upper_path )}')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Create Successfully!\n"
     ]
    }
   ],
   "source": [
    "create_folder('../data/temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temp_record_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_record_sdf(sdf:SparkSession, path = '../data/temp', overwrite = False):\n",
    "    '''\n",
    "Save current progress for future steps\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "sdf : spark dataframe\n",
    "path : str\n",
    "    Path to save data, defualt as `../data/temp`\n",
    "overwrite : bool\n",
    "    Set if cover the origin data, defualt False\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> temp_record_sdf(sdf, path='../data/temp')\n",
    ">>> temp_record_sdf(sdf, path='../data/temp')\n",
    ">>> temp_record_sdf(sdf, path='../data/temp', overwrite=True)\n",
    "|> Waitting for saving...\n",
    "|> Save Successfully!\n",
    "--\n",
    "|> Waitting for saving...\n",
    "|> The folder already exist! Change the attr `overwrite` to cover the origin data.\n",
    "-- \n",
    "|> Waitting for saving...\n",
    "|> Save Successfully!\n",
    "\n",
    ">>> print(os.listdir( '../data' ))\n",
    ">>> print(os.path.isfile( '../data/temp.parquet' ))\n",
    ">>> temp_record_sdf(sdf, path='../data/temp.parquet')\n",
    ">>> temp_record_sdf(sdf, path='../data/temp.parquet', overwrite=True)\n",
    "['tables', '.gitkeep', 'README.md', 'temp.parquet', 'curated']\n",
    "--\n",
    "True\n",
    "--\n",
    "|> The name duplicated with a file!\n",
    "   Change the name or change the attr `overwrite` to cover the origin data.\n",
    "--\n",
    "|> Waitting for saving...\n",
    "|> Save Successfully!\n",
    "    '''\n",
    "\n",
    "\n",
    "    # folder should not already exist\n",
    "    if not os.path.exists(path):\n",
    "        print('|> Waitting for saving...')\n",
    "        sdf.write.parquet(path)\n",
    "        print('|> Save Successfully!')\n",
    "    \n",
    "    # if the folder aleady created, the print out the files under this folder\n",
    "    elif os.path.isdir(path):\n",
    "        try:\n",
    "            print('|> Waitting for saving...')\n",
    "            if (overwrite):\n",
    "                sdf.write.partitionBy('order_datetime').parquet(path, mode = 'overwrite')\n",
    "            else:\n",
    "                sdf.write.parquet(path)\n",
    "            print('|> Save Successfully!')\n",
    "        except Exception:\n",
    "            print('|> The folder already exist! Change the attr `overwrite` to cover the origin data.')\n",
    "    \n",
    "    # the name of the new folder is the same as a file already exist under the upper folder\n",
    "    elif os.path.isfile(path):\n",
    "        if (overwrite):\n",
    "            print('|> Waitting for saving...')\n",
    "            sdf.write.parquet(path, mode = 'overwrite')\n",
    "            print('|> Save Successfully!')\n",
    "        else:\n",
    "            print(f'|> The name duplicated with a file!\\n   Change the name or change the attr `overwrite` to cover the origin data.')\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- dollar_value: double (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Waitting for saving...\n",
      "|> The folder already exist! Change the attr `overwrite` to cover the origin data.\n"
     ]
    }
   ],
   "source": [
    "temp_record_sdf(sdf, path='../data/temp')\n",
    "# temp_record_sdf(sdf, path='../data/temp')\n",
    "# temp_record_sdf(sdf, path='../data/temp', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temp_record_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_record_query(sql_query:SparkSession.sql, *cols,\\\n",
    "    path='../data/temp', overwrite = False):\n",
    "    '''\n",
    "Save current progress for future steps\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "sql_query : spark sql query\n",
    "*cols : 'ColumnsOrName'\n",
    "    Name of columns.\n",
    "path : str\n",
    "    Path to save data, defualt as `../data/temp`\n",
    "overwrite : bool\n",
    "    Set if cover the origin data, default False\n",
    "\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> sql_query = sdf.orderBy('merchant_abn')\n",
    ">>> temp_record_query(sql_query, 'name', 'tags', 'merchant_abn')\n",
    "|> Waitting for saving...\n",
    "|> Save Successfully!\n",
    "    '''\n",
    "    # convert to spark dataframe and save\n",
    "    temp_record_sdf(sql_query.toDF(*cols), path=path, overwrite=overwrite)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: The number of columns doesn't match.\nOld column names (5): user_id, merchant_abn, dollar_value, order_id, order_datetime\nNew column names (3): name, tags, merchant_abn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/8f9gkhvs7wnf17_m27m6s_fc0000gn/T/ipykernel_30274/1527791762.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msql_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'merchant_abn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtemp_record_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tags'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'merchant_abn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/px/8f9gkhvs7wnf17_m27m6s_fc0000gn/T/ipykernel_30274/3681963166.py\u001b[0m in \u001b[0;36mtemp_record_query\u001b[0;34m(sql_query, path, overwrite, *cols)\u001b[0m\n\u001b[1;32m     23\u001b[0m     '''\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# convert to spark dataframe and save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtemp_record_sdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3147\u001b[0m         \"\"\"\n\u001b[0;32m-> 3148\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: The number of columns doesn't match.\nOld column names (5): user_id, merchant_abn, dollar_value, order_id, order_datetime\nNew column names (3): name, tags, merchant_abn"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "sql_query = sdf.orderBy('merchant_abn')\n",
    "temp_record_query(sql_query, 'name', 'tags', 'merchant_abn', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transfor between DataFrame and RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# DataFrame to RDD\n",
    "srdd = sdf.rdd.map ( lambda p: Row(user_id=p.user_id, merchant_abn=p.merchant_abn, \\\n",
    "    dollar_value=p.dollar_value, order_id=p.order_id, order_datetime=p.order_datetime) )\n",
    "\n",
    "# RDD to DataFrame\n",
    "sdf = spark.createDataFrame( srdd )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), \"scripts\")\n",
    "sys.path.append(path)\n",
    "from utils import Utils_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/runyuyang/Desktop/generic-buy-now-pay-later-project-group-24/notebooks',\n",
       " '/private/var/folders/px/8f9gkhvs7wnf17_m27m6s_fc0000gn/T/spark-5a426a32-7419-4b8f-bd90-2ebf41846fbf/userFiles-08d20a81-81b7-4a4a-841f-513a1ce75fa3',\n",
       " '/Users/runyuyang/.vscode/extensions/ms-toolsai.jupyter-2022.6.1201981810/pythonFiles',\n",
       " '/Users/runyuyang/.vscode/extensions/ms-toolsai.jupyter-2022.6.1201981810/pythonFiles/lib/python',\n",
       " '/Library/Frameworks/Python.framework/Versions/3.7/lib/python37.zip',\n",
       " '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7',\n",
       " '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload',\n",
       " '',\n",
       " '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages',\n",
       " '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/Users/runyuyang/.ipython',\n",
       " 'scripts',\n",
       " '/Users/runyuyang/Desktop/generic-buy-now-pay-later-project-group-24/scripts']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
